apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-webui-deployment
  namespace: vllm-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: open-webui-server
  template:
    metadata:
      labels:
        app: open-webui-server
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64 #the container image runs only on amd64 and X86, It doesnâ€™t support arm64
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: open-webui-server
      tolerations:
      - key: "workload"
        operator: "Equal"
        value: "cpu"
        effect: "NoSchedule"
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      automountServiceAccountToken: false
      containers:
      - name: open-webui
        image: kopi/openwebui@sha256:38a883697e6ee80a0cf2505e7a90c9f24d1ddf6dc8c6edc9bc58cbb729a64f2f
        imagePullPolicy: Always
        securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - NET_RAW
            seccompProfile:
              type: RuntimeDefault
        resources:
          requests:
            cpu: "200m"
            memory: "1Gi"
          limits:
            cpu: "500m"
            memory: "2Gi"
        env:
        - name: WEBUI_AUTH
          value: "False"
        - name: OPENAI_API_KEY
          value: "xxx"
        - name: OPENAI_API_BASE_URLS
          value: "http://vllm-service/v1;http://vllm-service-karsh/v1"

